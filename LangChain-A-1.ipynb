{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828b1d15",
   "metadata": {},
   "source": [
    "### Basic Usages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d36ad",
   "metadata": {},
   "source": [
    "### LangChains only used on Running/Deployed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a330b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a686ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d646c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    model_kwargs = {'temperature':0.9}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b661655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52d5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurent_template = \"\"\"I want you to act as naming consultant for new restaurents.Return a list of restaurent names. Each name should be short, catchy and easy to remember. It should relate to type of restaurent.What are the good names for a resutaurent that is {restaurent_description}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d948f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables = ['restaurent_description'],\n",
    "    template = restaurent_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "644c601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81af332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want you to act as naming consultant for new restaurents.Return a list of restaurent names. Each name should be short, catchy and easy to remember. It should relate to type of restaurent.What are the good names for a resutaurent that is Hotel only veg recipes\n",
      "\n",
      "1. Green Haven: This name implies a peaceful, vegetarian retreat within a hotel, emphasizing the serene and healthy nature of the restaurant's offerings.\n",
      "2. Verdant Vista: This name suggests a beautiful, vegetarian view from the hotel restaurant, highlighting the visual appeal and freshness of the dishes.\n",
      "3. Plant Paradise: This name evokes an exotic, vegetarian oasis within the hotel, emphasizing the unique and delicious aspects of\n"
     ]
    }
   ],
   "source": [
    "description = \"Hotel only veg recipes\"\n",
    "print(chain.run(description))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c0088",
   "metadata": {},
   "source": [
    "### Few shot Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27ab6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae175d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\" : \"sad\"},\n",
    "    {\"word\": \"bad\", \"antonym\" : \"good\"},\n",
    "    {\"word\": \"tall\", \"antonym\" : \"small\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e1c4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_formatter_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\",\"antonym\"],\n",
    "    template = example_formatter_template\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix = \"Give the antonym of every input\",\n",
    "    suffix = \"Word: {input} \\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "770d2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "348978ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "\n",
      "\n",
      "Word: bad\n",
      "Antonym: good\n",
      "\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym: small\n",
      "\n",
      "\n",
      "Word: Big \n",
      "Antonym: Small\n",
      "\n",
      "\n",
      "Word: dark\n",
      "Antonym: light\n",
      "\n",
      "\n",
      "\n",
      "Word: female\n",
      "Antonym: male\n",
      "\n",
      "\n",
      "Word: old\n",
      "Antonym: young\n",
      "\n",
      "\n",
      "Word: hot\n",
      "Antonym: cold\n",
      "\n",
      "\n",
      "Word: thin\n",
      "Antonym: thick\n",
      "\n",
      "\n",
      "Word: good\n",
      "Antonym: bad\n",
      "\n",
      "\n",
      "Word: important\n",
      "Antonym: unimportant\n",
      "\n",
      "\n",
      "Word: surprising\n",
      "Antonym: unsurprising\n",
      "\n",
      "\n",
      "Word: right\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=few_shot_prompt)\n",
    "print(chain.run(\"Big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b27101",
   "metadata": {},
   "source": [
    "### Types of Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6badebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generic Chains\n",
    "# 2. Utility Chains\n",
    "# 3. Asynchronous Chains\n",
    "# 4. Sequencial Chains\n",
    "# 5. PAL Chains\n",
    "# 6. SQL Database Chains\n",
    "# 7. Transformation Chains\n",
    "# 8. Bash Chain\n",
    "# 9. Requests Chain\n",
    "# 10. API Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75620f63",
   "metadata": {},
   "source": [
    "##### 1. Generic Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dabd353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"India a country\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "690b5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72be1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08773145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm  \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/langchain_core/_api/deprecation.py:203\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     emit_warning()\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "llm  = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    temperature=0,\n",
    "    max_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03a307be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_extraction_prompt = PromptTemplate(\n",
    "    input_variables = [\"text_input\"],\n",
    "    template = \"Extract the key facts out of this tex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2686092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm,prompt=fact_extraction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cf730a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    res = chain.run(article)\n",
    "    print(res)\n",
    "except IndexError as msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8116b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb651834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
