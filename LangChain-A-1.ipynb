{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828b1d15",
   "metadata": {},
   "source": [
    "### Basic Usages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d36ad",
   "metadata": {},
   "source": [
    "### LangChains only used on Running/Deployed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a330b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain-community langchain-core\n",
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a686ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d646c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`. Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.\n\nFor further information visit https://errors.pydantic.dev/2.7/u/root-validator-pre-skip",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHub\n\u001b[1;32m      2\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceHub(\n\u001b[1;32m      3\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.9\u001b[39m}\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m/packages/Transformers_GPU/7c25e58f-77ec-42b5-8776-4b6530aa9315/3.8/langchain/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mabsolute()\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m _f:\n\u001b[1;32m      6\u001b[0m     __version__ \u001b[38;5;241m=\u001b[39m _f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     LLMChain,\n\u001b[1;32m     10\u001b[0m     LLMMathChain,\n\u001b[1;32m     11\u001b[0m     MRKLChain,\n\u001b[1;32m     12\u001b[0m     PythonChain,\n\u001b[1;32m     13\u001b[0m     ReActChain,\n\u001b[1;32m     14\u001b[0m     SelfAskWithSearchChain,\n\u001b[1;32m     15\u001b[0m     SerpAPIChain,\n\u001b[1;32m     16\u001b[0m     SQLDatabaseChain,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wikipedia\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfaiss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n",
      "File \u001b[0;32m/packages/Transformers_GPU/7c25e58f-77ec-42b5-8776-4b6530aa9315/3.8/langchain/chains/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Chains are easily reusable components which can be linked together.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_math\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMMathChain\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrkl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MRKLChain\n",
      "File \u001b[0;32m/packages/Transformers_GPU/7c25e58f-77ec-42b5-8776-4b6530aa9315/3.8/langchain/chains/llm.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Extra\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chain\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasePrompt\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLLMChain\u001b[39;00m(Chain, BaseModel):\n",
      "File \u001b[0;32m/packages/Transformers_GPU/7c25e58f-77ec-42b5-8776-4b6530aa9315/3.8/langchain/llms/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Wrappers on top of large language models APIs.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcohere\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cohere\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHub\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
      "File \u001b[0;32m/packages/Transformers_GPU/7c25e58f-77ec-42b5-8776-4b6530aa9315/3.8/langchain/llms/cohere.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enforce_stop_tokens\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCohere\u001b[39;00m(BaseModel, LLM):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around Cohere large language models.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    To use, you should have the ``cohere`` python package installed, and the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m            cohere = Cohere(model=\"gptd-instruct-tft\", cohere_api_key=\"my-api-key\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     client: Any  \u001b[38;5;66;03m#: :meta private:\u001b[39;00m\n",
      "File \u001b[0;32m/packages/Transformers_GPU/7c25e58f-77ec-42b5-8776-4b6530aa9315/3.8/langchain/llms/cohere.py:54\u001b[0m, in \u001b[0;36mCohere\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Configuration for this pydantic object.\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     extra \u001b[38;5;241m=\u001b[39m Extra\u001b[38;5;241m.\u001b[39mforbid\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;129m@root_validator\u001b[39m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate that api key and python package exists in environment.\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     cohere_api_key \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcohere_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/packages/Transformers_GPU/7c25e58f-77ec-42b5-8776-4b6530aa9315/3.8/pydantic/deprecated/class_validators.py:249\u001b[0m, in \u001b[0;36mroot_validator\u001b[0;34m(pre, skip_on_failure, allow_reuse, *__args)\u001b[0m\n\u001b[1;32m    247\u001b[0m mode: Literal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pre \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m skip_on_failure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    252\u001b[0m         code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot-validator-pre-skip\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    255\u001b[0m wrap \u001b[38;5;241m=\u001b[39m partial(_decorators_v1\u001b[38;5;241m.\u001b[39mmake_v1_generic_root_validator, pre\u001b[38;5;241m=\u001b[39mpre)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdec\u001b[39m(f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mclassmethod\u001b[39m[Any, Any, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstaticmethod\u001b[39m[Any, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "\u001b[0;31mPydanticUserError\u001b[0m: If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`. Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.\n\nFor further information visit https://errors.pydantic.dev/2.7/u/root-validator-pre-skip"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    model_kwargs = {'temperature':0.9}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b661655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurent_template = \"\"\"I want you to act as naming consultant for new restaurents.Return a list of restaurent names. Each name should be short, catchy and easy to remember. It should relate to type of restaurent.What are the good names for a resutaurent that is {restaurent_description}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables = ['restaurent_description'],\n",
    "    template = restaurent_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"Hotel only veg recipes\"\n",
    "print(chain.run(description))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c0088",
   "metadata": {},
   "source": [
    "### Few shot Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae175d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\" : \"sad\"},\n",
    "    {\"word\": \"bad\", \"antonym\" : \"good\"},\n",
    "    {\"word\": \"tall\", \"antonym\" : \"small\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e1c4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_formatter_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\",\"antonym\"],\n",
    "    template = example_formatter_template\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix = \"Give the antonym of every input\",\n",
    "    suffix = \"Word: {input} \\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "770d2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "348978ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "\n",
      "\n",
      "Word: bad\n",
      "Antonym: good\n",
      "\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym: small\n",
      "\n",
      "\n",
      "Word: Big \n",
      "Antonym: Small\n",
      "\n",
      "\n",
      "Word: Hot\n",
      "Antonym: Cold\n",
      "\n",
      "Word: Left\n",
      "Antonym: Right\n",
      "\n",
      "Word: Up\n",
      "Antonym: Down\n",
      "\n",
      "Word: Right\n",
      "Antonym: Wrong\n",
      "\n",
      "Word: In\n",
      "Antonym: Out\n",
      "\n",
      "Word: Reach\n",
      "Antonym: Fail\n",
      "\n",
      "Word: Add\n",
      "Antonym: Subtract\n",
      "\n",
      "Word: Open\n",
      "Antonym: Close\n",
      "\n",
      "Word: Normal\n",
      "Antonym: Abnormal\n",
      "\n",
      "Word:\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=few_shot_prompt)\n",
    "print(chain.run(\"Big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b27101",
   "metadata": {},
   "source": [
    "### Types of Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6badebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generic Chains\n",
    "# 2. Utility Chains\n",
    "# 3. Asynchronous Chains\n",
    "# 4. Sequencial Chains\n",
    "# 5. PAL Chains\n",
    "# 6. SQL Database Chains\n",
    "# 7. Transformation Chains\n",
    "# 8. Bash Chain\n",
    "# 9. Requests Chain\n",
    "# 10. API Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75620f63",
   "metadata": {},
   "source": [
    "##### 1. Generic Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dabd353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"India a country\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "690b5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03a307be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_extraction_prompt = PromptTemplate(\n",
    "    input_variables = [\"text_input\"],\n",
    "    template = \"Extract the key facts out of this tex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2686092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm,prompt=fact_extraction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5cf730a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    res = chain.run(article)\n",
    "    print(res)\n",
    "except IndexError as msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8116b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb651834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
